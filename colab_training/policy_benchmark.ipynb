{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# \ud83d\udcca Comprehensive Policy Benchmarking\n",
        "\n",
        "This notebook performs **fair, unbiased comparisons** between all policies:\n",
        "\n",
        "| Policy Type | Policies |\n",
        "|-------------|----------|\n",
        "| **RL Models** | Model 1 (Levels 1-3), Model 2 (Extreme) |\n",
        "| **Classical** | TBS, BaseStock, DoNothing |\n",
        "| **Advanced** | PIL, DIP, PEIP, VectorBS |\n",
        "\n",
        "**Methodology:**\n",
        "- All policies evaluated on the **exact same environment instances**\n",
        "- Same random seeds for fair demand/spoilage realization\n",
        "- Comprehensive metrics: cost, fill rate, **spoilage rate**, orders\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## 1\ufe0f\u20e3 Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install stable-baselines3 gymnasium numpy pandas matplotlib seaborn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# RL imports\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "\n",
        "# Project imports\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from colab_training.gym_env import PerishableInventoryGymWrapper, RewardConfig\n",
        "from colab_training.environment_suite import (\n",
        "    get_canonical_suite, build_environment_from_config\n",
        ")\n",
        "from colab_training.benchmark import (\n",
        "    get_tbs_policy_for_env, get_basestock_policy_for_env,\n",
        "    get_pil_policy_for_env, get_dip_policy_for_env,\n",
        "    get_peip_policy_for_env, get_vector_bs_policy_for_env,\n",
        "    get_all_policies_for_env, AVAILABLE_BASELINES\n",
        ")\n",
        "from perishable_inventory_mdp.policies import DoNothingPolicy\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (14, 7)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# All policies we will benchmark\n",
        "ALL_POLICIES = ['RL', 'TBS', 'BaseStock', 'DoNothing', 'PIL', 'DIP', 'PEIP', 'VectorBS']\n",
        "\n",
        "print('\u2705 All imports successful!')\n",
        "print(f'Available baselines: {AVAILABLE_BASELINES}')\n",
        "print(f'Policies to benchmark: {ALL_POLICIES}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## 2\ufe0f\u20e3 Configuration\n",
        "\n",
        "Configure your RL model paths and evaluation parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - EDIT THESE PATHS TO YOUR MODELS\n",
        "# ============================================================\n",
        "\n",
        "# RL Model paths\n",
        "RL_MODEL_LEVELS_1_3 = 'logs/best_model/best_model.zip'  # For simple, moderate, complex\n",
        "RL_MODEL_EXTREME = 'logs/extreme_model/best_model.zip'  # For extreme level\n",
        "\n",
        "# Evaluation parameters\n",
        "N_EPISODES = 10          # Episodes per environment (increase for more accuracy)\n",
        "N_ENVS_PER_LEVEL = 10    # Environments per complexity level\n",
        "MAX_STEPS = 500          # Max steps per episode\n",
        "RANDOM_SEED = 42         # For reproducibility\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = Path('benchmark_results')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f'\ud83d\udcc1 Results will be saved to: {OUTPUT_DIR.absolute()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "models-header"
      },
      "source": [
        "## 3\ufe0f\u20e3 Load RL Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-models"
      },
      "outputs": [],
      "source": [
        "# Load RL models\n",
        "print('\ud83d\udce6 Loading RL Models...')\n",
        "\n",
        "try:\n",
        "    model_levels_1_3 = PPO.load(RL_MODEL_LEVELS_1_3)\n",
        "    print(f'  \u2705 Model (Levels 1-3): {RL_MODEL_LEVELS_1_3}')\n",
        "except FileNotFoundError:\n",
        "    model_levels_1_3 = None\n",
        "    print(f'  \u26a0\ufe0f Model not found: {RL_MODEL_LEVELS_1_3}')\n",
        "\n",
        "try:\n",
        "    model_extreme = PPO.load(RL_MODEL_EXTREME)\n",
        "    print(f'  \u2705 Model (Extreme): {RL_MODEL_EXTREME}')\n",
        "except FileNotFoundError:\n",
        "    model_extreme = None\n",
        "    print(f'  \u26a0\ufe0f Model not found: {RL_MODEL_EXTREME}')\n",
        "\n",
        "if model_levels_1_3 is None and model_extreme is None:\n",
        "    print('\\n\u26a0\ufe0f WARNING: No RL models found! Will benchmark baselines only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env-header"
      },
      "source": [
        "## 4\ufe0f\u20e3 Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env-setup"
      },
      "outputs": [],
      "source": [
        "# Load environment suite\n",
        "suite = get_canonical_suite()\n",
        "print(f'\ud83d\udcca Loaded {len(suite)} environments')\n",
        "\n",
        "# Display complexity distribution\n",
        "complexity_counts = {}\n",
        "for level in ['simple', 'moderate', 'complex', 'extreme']:\n",
        "    configs = suite.get_by_complexity(level)\n",
        "    complexity_counts[level] = len(configs)\n",
        "    print(f'  {level.upper()}: {len(configs)} environments')\n",
        "\n",
        "# Reward configuration\n",
        "reward_config = RewardConfig()\n",
        "\n",
        "def create_env(config, seed=None):\n",
        "    '''Create gym environment from config with optional seed.'''\n",
        "    mdp = build_environment_from_config(config)\n",
        "    env = PerishableInventoryGymWrapper(mdp=mdp, reward_config=reward_config)\n",
        "    env = TimeLimit(env, max_episode_steps=MAX_STEPS)\n",
        "    env = Monitor(env)\n",
        "    if seed is not None:\n",
        "        env.reset(seed=seed)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval-header"
      },
      "source": [
        "## 5\ufe0f\u20e3 Evaluation Framework\n",
        "\n",
        "Fair evaluation: all policies tested on **identical** environment instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval-functions"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EpisodeMetrics:\n",
        "    '''Metrics from a single episode.'''\n",
        "    total_cost: float\n",
        "    holding_cost: float\n",
        "    backorder_cost: float\n",
        "    spoilage_cost: float\n",
        "    ordering_cost: float\n",
        "    total_demand: float\n",
        "    total_sales: float\n",
        "    total_spoilage: float\n",
        "    total_orders: float\n",
        "    total_arrivals: float  # NEW: for spoilage rate calculation\n",
        "    fill_rate: float\n",
        "    spoilage_rate: float    # NEW: spoilage rate metric\n",
        "    avg_inventory: float\n",
        "\n",
        "@dataclass\n",
        "class PolicyResult:\n",
        "    '''Aggregated results for a policy.'''\n",
        "    policy_name: str\n",
        "    env_id: str\n",
        "    complexity: str\n",
        "    episodes: List[EpisodeMetrics] = field(default_factory=list)\n",
        "    \n",
        "    @property\n",
        "    def mean_cost(self):\n",
        "        return np.mean([e.total_cost for e in self.episodes])\n",
        "    \n",
        "    @property\n",
        "    def std_cost(self):\n",
        "        return np.std([e.total_cost for e in self.episodes])\n",
        "    \n",
        "    @property\n",
        "    def mean_fill_rate(self):\n",
        "        return np.mean([e.fill_rate for e in self.episodes])\n",
        "    \n",
        "    @property\n",
        "    def mean_spoilage(self):\n",
        "        return np.mean([e.total_spoilage for e in self.episodes])\n",
        "    \n",
        "    @property\n",
        "    def mean_spoilage_rate(self):  # NEW\n",
        "        return np.mean([e.spoilage_rate for e in self.episodes])\n",
        "\n",
        "print('\u2705 Data classes defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-episode"
      },
      "outputs": [],
      "source": [
        "def run_episode(policy, env, is_rl_model=False):\n",
        "    '''Run single episode and collect metrics including spoilage rate.'''\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    \n",
        "    total_cost = 0\n",
        "    holding_cost = 0\n",
        "    backorder_cost = 0\n",
        "    spoilage_cost = 0\n",
        "    ordering_cost = 0\n",
        "    total_demand = 0\n",
        "    total_sales = 0\n",
        "    total_spoilage = 0\n",
        "    total_orders = 0\n",
        "    total_arrivals = 0  # Track arrivals for spoilage rate\n",
        "    inventory_sum = 0\n",
        "    steps = 0\n",
        "    \n",
        "    while not done and not truncated:\n",
        "        if is_rl_model:\n",
        "            action, _ = policy.predict(obs, deterministic=True)\n",
        "        else:\n",
        "            # Baseline policy - get MDP state from wrapper\n",
        "            wrapper = env\n",
        "            while hasattr(wrapper, 'env'):\n",
        "                if hasattr(wrapper, 'current_state'):\n",
        "                    break\n",
        "                wrapper = wrapper.env\n",
        "            \n",
        "            if hasattr(wrapper, 'current_state') and wrapper.current_state is not None:\n",
        "                state = wrapper.current_state\n",
        "                mdp = wrapper.mdp\n",
        "                action_dict = policy.get_action(state, mdp)\n",
        "                # Convert dict action to array\n",
        "                action = np.zeros(env.action_space.shape[0])\n",
        "                if hasattr(wrapper, 'supplier_order') and hasattr(wrapper, 'supplier_action_bins'):\n",
        "                    for i, sid in enumerate(wrapper.supplier_order):\n",
        "                        qty = action_dict.get(sid, 0)\n",
        "                        bins = wrapper.supplier_action_bins[sid]\n",
        "                        action[i] = min(range(len(bins)), key=lambda x: abs(bins[x] - qty))\n",
        "            else:\n",
        "                action = np.zeros(env.action_space.shape[0])\n",
        "        \n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        \n",
        "        # Accumulate metrics from info\n",
        "        holding_cost += info.get('holding_cost', 0)\n",
        "        backorder_cost += info.get('shortage_cost', 0)\n",
        "        spoilage_cost += info.get('spoilage_cost', 0)\n",
        "        ordering_cost += info.get('procurement_cost', 0)\n",
        "        total_demand += info.get('demand', 0)\n",
        "        total_sales += info.get('sales', 0)\n",
        "        total_spoilage += info.get('spoilage', 0)\n",
        "        \n",
        "        # Track orders/arrivals for spoilage rate\n",
        "        orders = info.get('orders', {})\n",
        "        if isinstance(orders, dict):\n",
        "            total_orders += sum(orders.values())\n",
        "        else:\n",
        "            total_orders += info.get('total_order', 0)\n",
        "        \n",
        "        # Arrivals approximation (orders placed lead_time steps ago)\n",
        "        # Using orders as proxy since arrivals = orders after lead time\n",
        "        total_arrivals += info.get('arrivals', info.get('total_order', sum(orders.values()) if isinstance(orders, dict) else 0))\n",
        "        \n",
        "        inventory_sum += info.get('inventory', 0)\n",
        "        \n",
        "        total_cost -= reward  # reward is negative cost\n",
        "        steps += 1\n",
        "    \n",
        "    fill_rate = total_sales / max(total_demand, 1e-6)\n",
        "    avg_inventory = inventory_sum / max(steps, 1)\n",
        "    \n",
        "    # Calculate spoilage rate: spoiled / (arrivals or orders or inventory throughput)\n",
        "    # Best proxy: spoiled / total_orders if arrivals not tracked\n",
        "    spoilage_rate = total_spoilage / max(total_orders, total_sales + total_spoilage, 1e-6)\n",
        "    \n",
        "    return EpisodeMetrics(\n",
        "        total_cost=total_cost,\n",
        "        holding_cost=holding_cost,\n",
        "        backorder_cost=backorder_cost,\n",
        "        spoilage_cost=spoilage_cost,\n",
        "        ordering_cost=ordering_cost,\n",
        "        total_demand=total_demand,\n",
        "        total_sales=total_sales,\n",
        "        total_spoilage=total_spoilage,\n",
        "        total_orders=total_orders,\n",
        "        total_arrivals=total_arrivals,\n",
        "        fill_rate=fill_rate,\n",
        "        spoilage_rate=spoilage_rate,\n",
        "        avg_inventory=avg_inventory\n",
        "    )\n",
        "\n",
        "print('\u2705 Episode runner ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fair-comparison"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_policies_fair(\n",
        "    config,\n",
        "    rl_model,\n",
        "    n_episodes: int = 10,\n",
        "    seed: int = 42\n",
        ") -> Dict[str, PolicyResult]:\n",
        "    '''\n",
        "    Evaluate ALL 7+ policies on the SAME environment with SAME seeds.\n",
        "    \n",
        "    Policies: RL, TBS, BaseStock, DoNothing, PIL, DIP, PEIP, VectorBS\n",
        "    '''\n",
        "    results = {}\n",
        "    env_id = config.env_id\n",
        "    complexity = config.complexity\n",
        "    \n",
        "    # Create base environment for policy creation\n",
        "    base_env = create_env(config, seed=seed)\n",
        "    \n",
        "    # Build policy dictionary with ALL policies\n",
        "    policies = {}\n",
        "    \n",
        "    # RL model (if available)\n",
        "    if rl_model is not None:\n",
        "        policies['RL'] = (rl_model, True)\n",
        "    \n",
        "    # TBS\n",
        "    try:\n",
        "        policies['TBS'] = (get_tbs_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f TBS unavailable: {e}')\n",
        "    \n",
        "    # BaseStock\n",
        "    try:\n",
        "        policies['BaseStock'] = (get_basestock_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f BaseStock unavailable: {e}')\n",
        "    \n",
        "    # DoNothing (always available)\n",
        "    policies['DoNothing'] = (DoNothingPolicy(), False)\n",
        "    \n",
        "    # PIL\n",
        "    try:\n",
        "        policies['PIL'] = (get_pil_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f PIL unavailable: {e}')\n",
        "    \n",
        "    # DIP\n",
        "    try:\n",
        "        policies['DIP'] = (get_dip_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f DIP unavailable: {e}')\n",
        "    \n",
        "    # PEIP\n",
        "    try:\n",
        "        policies['PEIP'] = (get_peip_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f PEIP unavailable: {e}')\n",
        "    \n",
        "    # VectorBS\n",
        "    try:\n",
        "        policies['VectorBS'] = (get_vector_bs_policy_for_env(base_env), False)\n",
        "    except Exception as e:\n",
        "        print(f'    \u26a0\ufe0f VectorBS unavailable: {e}')\n",
        "    \n",
        "    base_env.close()\n",
        "    \n",
        "    print(f'    Evaluating {len(policies)} policies: {list(policies.keys())}')\n",
        "    \n",
        "    # Evaluate each policy with SAME seeds for each episode\n",
        "    for policy_name, (policy, is_rl) in policies.items():\n",
        "        result = PolicyResult(policy_name=policy_name, env_id=env_id, complexity=complexity)\n",
        "        \n",
        "        for ep in range(n_episodes):\n",
        "            episode_seed = seed + ep\n",
        "            env = create_env(config, seed=episode_seed)\n",
        "            \n",
        "            try:\n",
        "                metrics = run_episode(policy, env, is_rl_model=is_rl)\n",
        "                result.episodes.append(metrics)\n",
        "            except Exception as e:\n",
        "                print(f'    \u26a0\ufe0f {policy_name} failed episode {ep}: {e}')\n",
        "            finally:\n",
        "                env.close()\n",
        "        \n",
        "        results[policy_name] = result\n",
        "    \n",
        "    return results\n",
        "\n",
        "print('\u2705 Fair comparison function ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benchmark-header"
      },
      "source": [
        "## 6\ufe0f\u20e3 Run Comprehensive Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-benchmarks"
      },
      "outputs": [],
      "source": [
        "# Run comprehensive benchmarks\n",
        "all_results = []\n",
        "\n",
        "print('=' * 70)\n",
        "print('\ud83d\udcca COMPREHENSIVE POLICY BENCHMARKING')\n",
        "print('=' * 70)\n",
        "print(f'Episodes per env: {N_EPISODES}')\n",
        "print(f'Environments per level: {N_ENVS_PER_LEVEL}')\n",
        "print(f'Policies: {ALL_POLICIES}')\n",
        "print('=' * 70)\n",
        "\n",
        "for complexity in ['simple', 'moderate', 'complex', 'extreme']:\n",
        "    configs = suite.get_by_complexity(complexity)\n",
        "    n_envs = min(N_ENVS_PER_LEVEL, len(configs))\n",
        "    \n",
        "    if n_envs == 0:\n",
        "        continue\n",
        "    \n",
        "    # Select appropriate RL model\n",
        "    if complexity == 'extreme':\n",
        "        rl_model = model_extreme\n",
        "        model_label = 'RL (Extreme)'\n",
        "    else:\n",
        "        rl_model = model_levels_1_3\n",
        "        model_label = 'RL (L1-3)'\n",
        "    \n",
        "    print(f'\\n\ud83d\udd0d {complexity.upper()} ({n_envs} environments)')\n",
        "    print(f'   Using: {model_label}')\n",
        "    print('-' * 50)\n",
        "    \n",
        "    for i, config in enumerate(configs[:n_envs]):\n",
        "        print(f'  [{i+1}/{n_envs}] {config.env_id}...')\n",
        "        \n",
        "        try:\n",
        "            results = evaluate_all_policies_fair(\n",
        "                config=config,\n",
        "                rl_model=rl_model,\n",
        "                n_episodes=N_EPISODES,\n",
        "                seed=RANDOM_SEED\n",
        "            )\n",
        "            \n",
        "            for policy_name, result in results.items():\n",
        "                all_results.append({\n",
        "                    'policy': policy_name,\n",
        "                    'env_id': config.env_id,\n",
        "                    'complexity': complexity,\n",
        "                    'mean_cost': result.mean_cost,\n",
        "                    'std_cost': result.std_cost,\n",
        "                    'fill_rate': result.mean_fill_rate,\n",
        "                    'spoilage': result.mean_spoilage,\n",
        "                    'spoilage_rate': result.mean_spoilage_rate,  # NEW\n",
        "                    'n_episodes': len(result.episodes)\n",
        "                })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f'    FAILED: {e}')\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('\u2705 BENCHMARKING COMPLETE')\n",
        "print('=' * 70)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_results)\n",
        "print(f'\\nTotal evaluations: {len(df)}')\n",
        "print(f'Policies evaluated: {df[\"policy\"].unique().tolist()}')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-results"
      },
      "outputs": [],
      "source": [
        "# Save results\n",
        "results_path = OUTPUT_DIR / 'benchmark_results.csv'\n",
        "df.to_csv(results_path, index=False)\n",
        "print(f'\ud83d\udcbe Results saved to: {results_path}')\n",
        "\n",
        "# Save as JSON too\n",
        "df.to_json(OUTPUT_DIR / 'benchmark_results.json', orient='records', indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viz-header"
      },
      "source": [
        "## 7\ufe0f\u20e3 Comprehensive Visualizations\n",
        "\n",
        "All metrics: **Cost**, **Fill Rate**, **Spoilage Rate** across ALL policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-cost-complexity"
      },
      "outputs": [],
      "source": [
        "# 1. Mean Cost by Policy and Complexity\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "pivot = df.pivot_table(values='mean_cost', index='policy', columns='complexity', aggfunc='mean')\n",
        "cols = ['simple', 'moderate', 'complex', 'extreme']\n",
        "pivot = pivot[[c for c in cols if c in pivot.columns]]\n",
        "\n",
        "pivot.plot(kind='bar', ax=ax, width=0.8, edgecolor='white', linewidth=1.5)\n",
        "ax.set_xlabel('Policy', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Mean Total Cost', fontsize=12, fontweight='bold')\n",
        "ax.set_title('\ud83d\udcca Mean Cost by Policy and Complexity Level (All Policies)', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Complexity', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.0f', fontsize=8, padding=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'cost_by_complexity.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-fillrate"
      },
      "outputs": [],
      "source": [
        "# 2. Fill Rate by Policy and Complexity  \n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "pivot_fr = df.pivot_table(values='fill_rate', index='policy', columns='complexity', aggfunc='mean')\n",
        "cols = ['simple', 'moderate', 'complex', 'extreme']\n",
        "pivot_fr = pivot_fr[[c for c in cols if c in pivot_fr.columns]]\n",
        "\n",
        "pivot_fr.plot(kind='bar', ax=ax, width=0.8, edgecolor='white', linewidth=1.5)\n",
        "ax.set_xlabel('Policy', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Mean Fill Rate', fontsize=12, fontweight='bold')\n",
        "ax.set_title('\ud83d\udce6 Fill Rate by Policy and Complexity Level (All Policies)', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Complexity', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "ax.set_ylim(0, 1.1)\n",
        "ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.5, label='95% Target')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'fillrate_by_complexity.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-spoilage-rate"
      },
      "outputs": [],
      "source": [
        "# 3. Spoilage Rate by Policy and Complexity (NEW)\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "pivot_sr = df.pivot_table(values='spoilage_rate', index='policy', columns='complexity', aggfunc='mean')\n",
        "cols = ['simple', 'moderate', 'complex', 'extreme']\n",
        "pivot_sr = pivot_sr[[c for c in cols if c in pivot_sr.columns]]\n",
        "\n",
        "pivot_sr.plot(kind='bar', ax=ax, width=0.8, edgecolor='white', linewidth=1.5, colormap='Reds')\n",
        "ax.set_xlabel('Policy', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Mean Spoilage Rate', fontsize=12, fontweight='bold')\n",
        "ax.set_title('\ud83d\uddd1\ufe0f Spoilage Rate by Policy and Complexity Level (All Policies)', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Complexity', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "ax.axhline(y=0.05, color='green', linestyle='--', alpha=0.5, label='5% Target')\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.2f', fontsize=8, padding=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'spoilage_rate_by_complexity.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-boxplot"
      },
      "outputs": [],
      "source": [
        "# 4. Box Plot of Cost Distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "for idx, complexity in enumerate(['simple', 'moderate', 'complex', 'extreme']):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    subset = df[df['complexity'] == complexity]\n",
        "    \n",
        "    if len(subset) > 0:\n",
        "        subset.boxplot(column='mean_cost', by='policy', ax=ax)\n",
        "        ax.set_title(f'{complexity.upper()}', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Policy')\n",
        "        ax.set_ylabel('Mean Cost')\n",
        "        plt.sca(ax)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.suptitle('\ud83d\udcc8 Cost Distribution by Complexity Level (All Policies)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'cost_boxplots.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-heatmap"
      },
      "outputs": [],
      "source": [
        "# 5. Heatmaps of All Metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Cost heatmap\n",
        "pivot_cost = df.pivot_table(values='mean_cost', index='policy', columns='complexity', aggfunc='mean')\n",
        "sns.heatmap(pivot_cost, annot=True, fmt='.0f', cmap='RdYlGn_r', ax=axes[0])\n",
        "axes[0].set_title('Mean Cost (lower is better)', fontweight='bold')\n",
        "\n",
        "# Fill rate heatmap\n",
        "pivot_fr = df.pivot_table(values='fill_rate', index='policy', columns='complexity', aggfunc='mean')\n",
        "sns.heatmap(pivot_fr, annot=True, fmt='.2%', cmap='RdYlGn', ax=axes[1])\n",
        "axes[1].set_title('Fill Rate (higher is better)', fontweight='bold')\n",
        "\n",
        "# Spoilage rate heatmap (NEW)\n",
        "pivot_sr = df.pivot_table(values='spoilage_rate', index='policy', columns='complexity', aggfunc='mean')\n",
        "sns.heatmap(pivot_sr, annot=True, fmt='.2%', cmap='RdYlGn_r', ax=axes[2])\n",
        "axes[2].set_title('Spoilage Rate (lower is better)', fontweight='bold')\n",
        "\n",
        "plt.suptitle('\ud83d\udd25 Policy Performance Heatmaps (All Metrics)', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'performance_heatmaps.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-rl-vs-baseline"
      },
      "outputs": [],
      "source": [
        "# 6. RL vs Best Baseline Comparison\n",
        "summary = df.groupby(['complexity', 'policy'])['mean_cost'].mean().reset_index()\n",
        "\n",
        "comparison_data = []\n",
        "for complexity in ['simple', 'moderate', 'complex', 'extreme']:\n",
        "    subset = summary[summary['complexity'] == complexity]\n",
        "    if len(subset) == 0:\n",
        "        continue\n",
        "    \n",
        "    rl_cost = subset[subset['policy'] == 'RL']['mean_cost'].values\n",
        "    rl_cost = rl_cost[0] if len(rl_cost) > 0 else None\n",
        "    \n",
        "    baselines = subset[subset['policy'] != 'RL']\n",
        "    if len(baselines) > 0:\n",
        "        best_baseline = baselines.loc[baselines['mean_cost'].idxmin()]\n",
        "        comparison_data.append({\n",
        "            'complexity': complexity,\n",
        "            'RL_cost': rl_cost,\n",
        "            'best_baseline': best_baseline['policy'],\n",
        "            'best_baseline_cost': best_baseline['mean_cost'],\n",
        "            'improvement': (best_baseline['mean_cost'] - rl_cost) / best_baseline['mean_cost'] * 100 if rl_cost else None\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print('\\n\ud83d\udcca RL vs Best Baseline Summary:')\n",
        "print('=' * 70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "if len(comparison_df) > 0 and comparison_df['RL_cost'].notna().any():\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    \n",
        "    x = np.arange(len(comparison_df))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax.bar(x - width/2, comparison_df['RL_cost'].fillna(0), width, label='RL', color='#2ecc71')\n",
        "    bars2 = ax.bar(x + width/2, comparison_df['best_baseline_cost'], width, label='Best Baseline', color='#3498db')\n",
        "    \n",
        "    ax.set_xlabel('Complexity Level', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Mean Cost', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('\ud83c\udfc6 RL vs Best Baseline by Complexity', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(comparison_df['complexity'].str.upper())\n",
        "    ax.legend()\n",
        "    \n",
        "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
        "        if row['improvement'] and pd.notna(row['improvement']):\n",
        "            color = 'green' if row['improvement'] > 0 else 'red'\n",
        "            ax.annotate(f\"{row['improvement']:.1f}%\", \n",
        "                       xy=(i, max(row['RL_cost'] or 0, row['best_baseline_cost']) + 50),\n",
        "                       ha='center', fontsize=10, color=color, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / 'rl_vs_baseline.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-ranking"
      },
      "outputs": [],
      "source": [
        "# 7. Overall Policy Ranking with All Metrics\n",
        "ranking = df.groupby('policy').agg({\n",
        "    'mean_cost': ['mean', 'std'],\n",
        "    'fill_rate': 'mean',\n",
        "    'spoilage': 'mean',\n",
        "    'spoilage_rate': 'mean'  # NEW\n",
        "}).round(4)\n",
        "\n",
        "ranking.columns = ['Mean Cost', 'Std Cost', 'Fill Rate', 'Spoilage (units)', 'Spoilage Rate']\n",
        "ranking = ranking.sort_values('Mean Cost')\n",
        "\n",
        "print('\\n\ud83c\udfc6 OVERALL POLICY RANKING (by Mean Cost)')\n",
        "print('=' * 90)\n",
        "print(ranking.to_string())\n",
        "\n",
        "# Visualize ranking\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Cost ranking\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.8, len(ranking)))\n",
        "axes[0].barh(ranking.index, ranking['Mean Cost'], color=colors, edgecolor='white', linewidth=1.5)\n",
        "axes[0].set_xlabel('Mean Cost', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Cost (Lower = Better)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Fill rate ranking\n",
        "fr_sorted = ranking.sort_values('Fill Rate', ascending=True)\n",
        "axes[1].barh(fr_sorted.index, fr_sorted['Fill Rate'], color=plt.cm.Greens(np.linspace(0.3, 0.9, len(ranking))), edgecolor='white')\n",
        "axes[1].set_xlabel('Fill Rate', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Fill Rate (Higher = Better)', fontsize=12, fontweight='bold')\n",
        "axes[1].axvline(x=0.95, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Spoilage rate ranking (NEW)\n",
        "sr_sorted = ranking.sort_values('Spoilage Rate', ascending=False)\n",
        "axes[2].barh(sr_sorted.index, sr_sorted['Spoilage Rate'], color=plt.cm.Reds(np.linspace(0.3, 0.9, len(ranking))), edgecolor='white')\n",
        "axes[2].set_xlabel('Spoilage Rate', fontsize=11, fontweight='bold')\n",
        "axes[2].set_title('Spoilage Rate (Lower = Better)', fontsize=12, fontweight='bold')\n",
        "axes[2].axvline(x=0.05, color='green', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.suptitle('\ud83c\udfc6 Overall Policy Ranking - All Metrics', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'policy_ranking_all_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz-radar"
      },
      "outputs": [],
      "source": [
        "# 8. Radar Chart of Multi-Metric Performance\n",
        "from math import pi\n",
        "\n",
        "# Prepare data - include spoilage rate\n",
        "metrics = ['fill_rate', 'mean_cost', 'spoilage_rate']\n",
        "policy_means = df.groupby('policy')[metrics].mean()\n",
        "\n",
        "# Normalize metrics (invert cost and spoilage_rate so higher is better)\n",
        "normalized = policy_means.copy()\n",
        "normalized['mean_cost'] = 1 - (normalized['mean_cost'] - normalized['mean_cost'].min()) / (normalized['mean_cost'].max() - normalized['mean_cost'].min() + 1e-6)\n",
        "normalized['spoilage_rate'] = 1 - (normalized['spoilage_rate'] - normalized['spoilage_rate'].min()) / (normalized['spoilage_rate'].max() - normalized['spoilage_rate'].min() + 1e-6)\n",
        "\n",
        "# Create radar chart\n",
        "categories = ['Fill Rate', 'Cost Efficiency', 'Low Spoilage Rate']\n",
        "N = len(categories)\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(normalized)))\n",
        "\n",
        "for idx, (policy, row) in enumerate(normalized.iterrows()):\n",
        "    values = [row['fill_rate'], row['mean_cost'], row['spoilage_rate']]\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=policy, color=colors[idx])\n",
        "    ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, fontsize=11)\n",
        "ax.set_title('\ud83c\udfaf Multi-Metric Policy Comparison\\n(Higher = Better)', fontsize=14, fontweight='bold', y=1.08)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'radar_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stats-header"
      },
      "source": [
        "## 8\ufe0f\u20e3 Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stats"
      },
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "from scipy import stats as scipy_stats\n",
        "\n",
        "print('\ud83d\udcca STATISTICAL SUMMARY')\n",
        "print('=' * 90)\n",
        "\n",
        "# Summary statistics with all metrics\n",
        "summary_stats = df.groupby('policy').agg({\n",
        "    'mean_cost': ['count', 'mean', 'std', 'min', 'max'],\n",
        "    'fill_rate': ['mean', 'std'],\n",
        "    'spoilage_rate': ['mean', 'std']  # NEW\n",
        "}).round(4)\n",
        "\n",
        "print('\\n\ud83d\udcc8 Summary Statistics (All Metrics):')\n",
        "print(summary_stats.to_string())\n",
        "\n",
        "# Pairwise comparisons with RL\n",
        "print('\\n\\n\ud83d\udcc9 RL vs Baselines Statistical Tests:')\n",
        "print('-' * 90)\n",
        "\n",
        "rl_costs = df[df['policy'] == 'RL']['mean_cost'].values\n",
        "\n",
        "if len(rl_costs) > 0:\n",
        "    for policy in df['policy'].unique():\n",
        "        if policy == 'RL':\n",
        "            continue\n",
        "        \n",
        "        baseline_costs = df[df['policy'] == policy]['mean_cost'].values\n",
        "        \n",
        "        if len(baseline_costs) > 1 and len(rl_costs) > 1:\n",
        "            min_len = min(len(rl_costs), len(baseline_costs))\n",
        "            t_stat, p_value = scipy_stats.ttest_ind(rl_costs[:min_len], baseline_costs[:min_len])\n",
        "            \n",
        "            rl_mean = np.mean(rl_costs)\n",
        "            baseline_mean = np.mean(baseline_costs)\n",
        "            improvement = (baseline_mean - rl_mean) / baseline_mean * 100\n",
        "            \n",
        "            sig = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''\n",
        "            \n",
        "            print(f'{policy:12} | RL: {rl_mean:.1f} vs {baseline_mean:.1f} | '\n",
        "                  f'Diff: {improvement:+.1f}% | p={p_value:.4f} {sig}')\n",
        "else:\n",
        "    print('No RL results available for comparison')\n",
        "\n",
        "print('\\nSignificance levels: * p<0.05, ** p<0.01, *** p<0.001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-header"
      },
      "source": [
        "## 9\ufe0f\u20e3 Final Summary & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final-summary"
      },
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print('\\n' + '=' * 90)\n",
        "print('\ud83d\udccb BENCHMARK SUMMARY REPORT')\n",
        "print('=' * 90)\n",
        "\n",
        "print(f'\\n\ud83d\udcca Evaluation Statistics:')\n",
        "print(f'   Total evaluations: {len(df)}')\n",
        "print(f'   Policies tested: {df[\"policy\"].nunique()} ({df[\"policy\"].unique().tolist()})')\n",
        "print(f'   Environments tested: {df[\"env_id\"].nunique()}')\n",
        "print(f'   Episodes per evaluation: {N_EPISODES}')\n",
        "\n",
        "print(f'\\n\ud83c\udfc6 Policy Rankings:')\n",
        "print('\\n   BY COST (lower is better):')\n",
        "rankings_cost = df.groupby('policy')['mean_cost'].mean().sort_values()\n",
        "for i, (policy, cost) in enumerate(rankings_cost.items(), 1):\n",
        "    print(f'   {i}. {policy}: {cost:.1f}')\n",
        "\n",
        "print('\\n   BY FILL RATE (higher is better):')\n",
        "rankings_fr = df.groupby('policy')['fill_rate'].mean().sort_values(ascending=False)\n",
        "for i, (policy, fr) in enumerate(rankings_fr.items(), 1):\n",
        "    print(f'   {i}. {policy}: {fr:.2%}')\n",
        "\n",
        "print('\\n   BY SPOILAGE RATE (lower is better):')\n",
        "rankings_sr = df.groupby('policy')['spoilage_rate'].mean().sort_values()\n",
        "for i, (policy, sr) in enumerate(rankings_sr.items(), 1):\n",
        "    print(f'   {i}. {policy}: {sr:.2%}')\n",
        "\n",
        "print(f'\\n\ud83d\udcc1 Output Files:')\n",
        "for f in OUTPUT_DIR.glob('*'):\n",
        "    print(f'   {f}')\n",
        "\n",
        "print('\\n' + '=' * 90)\n",
        "print('\u2705 BENCHMARKING COMPLETE!')\n",
        "print('=' * 90)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}